{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b7e13b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.5.8-py3-none-any.whl (526 kB)\n",
      "\u001b[K     |████████████████████████████████| 526 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (3.10.0.0)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 55.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (4.61.2)\n",
      "Collecting tensorboard>=2.2.0\n",
      "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 47.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.7.0-py3-none-any.whl (396 kB)\n",
      "\u001b[K     |████████████████████████████████| 396 kB 31.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.* in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (1.21.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (21.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from pytorch-lightning) (5.4.1)\n",
      "Collecting future>=0.17.1\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 57.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-macosx_10_9_x86_64.whl (574 kB)\n",
      "\u001b[K     |████████████████████████████████| 574 kB 38.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (52.0.0.post20210125)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-macosx_10_9_x86_64.whl (3.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 61.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 17.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.43.0-cp38-cp38-macosx_10_10_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.3.3-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 62.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 32.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 60.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.19.3-cp38-cp38-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 56.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 63.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.10.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.5.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 51.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-macosx_10_9_x86_64.whl (36 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp38-cp38-macosx_10_9_x86_64.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp38-cp38-macosx_10_9_x86_64.whl (121 kB)\n",
      "\u001b[K     |████████████████████████████████| 121 kB 51.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.10-py3-none-any.whl (39 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=f33e53920df850c6f9f76261ff5ec0b179b912d17239a35b50fb30da3f0130f1\n",
      "  Stored in directory: /Users/f707729/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, multidict, frozenlist, cachetools, yarl, requests-oauthlib, importlib-metadata, google-auth, charset-normalizer, async-timeout, aiosignal, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, pyDeprecate, protobuf, markdown, grpcio, google-auth-oauthlib, fsspec, aiohttp, absl-py, torchmetrics, tensorboard, future, pytorch-lightning\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 cachetools-4.2.4 charset-normalizer-2.0.10 frozenlist-1.3.0 fsspec-2022.1.0 future-0.18.2 google-auth-2.3.3 google-auth-oauthlib-0.4.6 grpcio-1.43.0 importlib-metadata-4.10.1 markdown-3.3.6 multidict-5.2.0 oauthlib-3.1.1 protobuf-3.19.3 pyDeprecate-0.3.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch-lightning-1.5.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.7.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.7.0 werkzeug-2.0.2 yarl-1.7.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861f625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb092ed8",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09c55fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/f707729/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3437: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../../data/ner.csv\",\n",
    "     encoding = \"ISO-8859-1\",\n",
    "     error_bad_lines=False,\n",
    "     usecols=['sentence_idx', 'word', 'tag']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc879d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = df[df['sentence_idx'] != 'prev-lemma'].dropna(subset=['sentence_idx']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ebcee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    sentences, sentence_tags = [], []\n",
    "\n",
    "    for sid, group in tqdm(df.groupby(\"sentence_idx\")):\n",
    "        words = group[\"word\"].tolist()\n",
    "        tags = group[\"tag\"].tolist()\n",
    "        \n",
    "        assert len(words) == len(tags)\n",
    "        \n",
    "        sentences.append(words)\n",
    "        sentence_tags.append(tags)\n",
    "    \n",
    "    return sentences, sentence_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8459ea07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36684/36684 [00:03<00:00, 10078.43it/s]\n"
     ]
    }
   ],
   "source": [
    "sentences, tags = get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d653281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    list(zip(sentences[0], tags[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f03321",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "321210c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    word_2_idx, idx_2_word = {}, {}\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for row in tqdm(data):\n",
    "        tokenized_row = []\n",
    "        for i, word in enumerate(row):\n",
    "            if word not in word_2_idx:\n",
    "                word_id = len(word_2_idx)\n",
    "                word_2_idx[word] = word_id\n",
    "                idx_2_word[word_id] = word\n",
    "            tokenized_row.append(word_2_idx[word])\n",
    "        tokenized_data.append(tokenized_row)\n",
    "            \n",
    "    return tokenized_data, word_2_idx, idx_2_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b09d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36684/36684 [00:00<00:00, 103819.52it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences, word_2_idx, idx_2_word = tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df8018f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36684/36684 [00:00<00:00, 217283.76it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_tags, tag_2_idx, idx_2_tag = tokenize(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fc38cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Thousands', 'O', 0),\n",
       " (1, 'of', 'O', 0),\n",
       " (2, 'demonstrators', 'O', 0),\n",
       " (3, 'have', 'O', 0),\n",
       " (4, 'marched', 'O', 0),\n",
       " (5, 'through', 'O', 0),\n",
       " (6, 'London', 'B-geo', 1),\n",
       " (7, 'to', 'O', 0),\n",
       " (8, 'protest', 'O', 0),\n",
       " (9, 'the', 'O', 0),\n",
       " (10, 'war', 'O', 0),\n",
       " (11, 'in', 'O', 0),\n",
       " (12, 'Iraq', 'B-geo', 1),\n",
       " (13, 'and', 'O', 0),\n",
       " (14, 'demand', 'O', 0),\n",
       " (9, 'the', 'O', 0),\n",
       " (15, 'withdrawal', 'O', 0),\n",
       " (1, 'of', 'O', 0),\n",
       " (16, 'British', 'B-gpe', 2),\n",
       " (17, 'troops', 'O', 0),\n",
       " (18, 'from', 'O', 0),\n",
       " (19, 'that', 'O', 0),\n",
       " (20, 'country', 'O', 0),\n",
       " (21, '.', 'O', 0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokenized_sentences[0], sentences[0], tags[0], tokenized_tags[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8bc9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = len(word_2_idx)\n",
    "word_2_idx[\"<PAD>\"] = PAD_IDX\n",
    "idx_2_word[PAD_IDX] = \"<PAD>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0635b494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30173, 30173, 17, 17)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_2_idx), len(idx_2_word), len(tag_2_idx), len(idx_2_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cbe2f044",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-geo': 1,\n",
       " 'B-gpe': 2,\n",
       " 'B-per': 3,\n",
       " 'I-geo': 4,\n",
       " 'B-org': 5,\n",
       " 'I-org': 6,\n",
       " 'B-tim': 7,\n",
       " 'B-art': 8,\n",
       " 'I-art': 9,\n",
       " 'I-per': 10,\n",
       " 'I-gpe': 11,\n",
       " 'I-tim': 12,\n",
       " 'B-nat': 13,\n",
       " 'B-eve': 14,\n",
       " 'I-eve': 15,\n",
       " 'I-nat': 16}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80250a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([889973,  37525,  16392,  17011,   7409,  20184,  16537,  20193,\n",
       "          434,    280,  17382,    229,   6298,    226,    348,    297,\n",
       "           76])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = []\n",
    "for _ in tokenized_tags:\n",
    "    t.extend(_)\n",
    "np.bincount(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94715914",
   "metadata": {},
   "source": [
    "## Train Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0f8b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b644c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tokenized_sentences, tokenized_tags, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e14b9c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31181, 31181, 5503, 5503)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4ad6ac8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16303, 8),\n",
       " (59, 0),\n",
       " (14807, 0),\n",
       " (31, 0),\n",
       " (296, 0),\n",
       " (297, 0),\n",
       " (1343, 0),\n",
       " (1891, 0),\n",
       " (254, 0),\n",
       " (18, 0),\n",
       " (1839, 3),\n",
       " (8893, 10),\n",
       " (93, 0),\n",
       " (45, 0),\n",
       " (8592, 0),\n",
       " (1, 0),\n",
       " (9, 0),\n",
       " (8513, 5),\n",
       " (6223, 6),\n",
       " (21, 0),\n",
       " (16303, 8),\n",
       " (59, 0),\n",
       " (14807, 0),\n",
       " (31, 0),\n",
       " (296, 0),\n",
       " (297, 0),\n",
       " (1343, 0),\n",
       " (1891, 0),\n",
       " (254, 0),\n",
       " (18, 0),\n",
       " (1839, 3),\n",
       " (8893, 10),\n",
       " (93, 0),\n",
       " (45, 0),\n",
       " (8592, 0),\n",
       " (1, 0),\n",
       " (9, 0),\n",
       " (8513, 5),\n",
       " (6223, 6),\n",
       " (21, 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_train[0], y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5aa4e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        words = self.x[i]\n",
    "        tags = self.y[i]\n",
    "        \n",
    "        return torch.Tensor(words), torch.Tensor(tags)\n",
    "\n",
    "def collate(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    \n",
    "    sentences = torch.nn.utils.rnn.pad_sequence(sentences, batch_first=True, padding_value=word_2_idx[\"<PAD>\"]).long()\n",
    "    tags = torch.nn.utils.rnn.pad_sequence(tags, batch_first=True, padding_value=-1).long()\n",
    "    \n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac927085",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NERDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=64, collate_fn=collate)\n",
    "\n",
    "test_dataset = NERDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=64, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134768d",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0364d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a83a7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn import CnnNER\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68623e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"n_classes\": len(tag_2_idx),\n",
    "    \"n_embeddings\": len(word_2_idx),\n",
    "    \"embed_dims\": 50,\n",
    "    \"n_cnn_layers\": 4,\n",
    "    \"n_cnn_channels\": 64,\n",
    "    \"cnn_kernel_size\": 5,\n",
    "    \"cnn_padding\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaed5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CnnNER(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65553f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CnnNER(\n",
       "  (embedding_layer): Embedding(30173, 50)\n",
       "  (cnn_layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv1d(50, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf0b9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5de03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epochs, model):\n",
    "    def train_batch(x, y, train):\n",
    "        if train:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        preds = model(x)\n",
    "        loss = loss_fn(preds.transpose(1, 2), y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        preds = preds.argmax(dim=-1)\n",
    "\n",
    "        mask = (y >= 0)\n",
    "        y_masked = y[mask]\n",
    "        preds_masked = preds[mask]\n",
    "\n",
    "        corr = (preds_masked == y_masked).sum().item()\n",
    "        f1 = f1_score(y_masked, preds_masked, average='macro')\n",
    "\n",
    "        return preds_masked, y_masked, corr, y_masked.numel(), loss.item(), f1\n",
    "\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        train_corr = 0\n",
    "        train_total = 0\n",
    "        train_loss = 0.\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        train_loop = tqdm(train_dataloader, leave=False, position=0)\n",
    "        train_loop.set_description(f\"Epoch {e+1}\")\n",
    "        for batch_num, (x, y) in enumerate(train_loop):\n",
    "            batch_preds, batch_true, batch_corr, batch_total, batch_loss, f1 = train_batch(x, y, train=True)\n",
    "\n",
    "            train_corr += batch_corr\n",
    "            train_total += batch_total\n",
    "            train_loss += batch_loss\n",
    "            train_preds.extend(batch_preds.tolist())\n",
    "            train_true.extend(batch_true.tolist())\n",
    "\n",
    "            train_loop.set_postfix(acc=(batch_corr/batch_total) * 100., loss=batch_loss, f1=f1)\n",
    "\n",
    "        print(f\"Train Epoch {e+1} loss={train_loss/len(train_dataloader)} acc={train_corr/train_total * 100} f1={f1_score(train_true, train_preds, average='weighted')}\")\n",
    "\n",
    "        val_corr = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        val_loop = tqdm(test_dataloader, leave=False, position=0)\n",
    "        val_loop.set_description(f\"Epoch {e+1}\")\n",
    "        for batch_num, (x, y) in enumerate(val_loop):\n",
    "            batch_preds, batch_true, batch_corr, batch_total, batch_loss, f1 = train_batch(x, y, train=False)\n",
    "\n",
    "            val_corr += batch_corr\n",
    "            val_total += batch_total\n",
    "            val_loss += batch_loss\n",
    "            val_preds.extend(batch_preds.tolist())\n",
    "            val_true.extend(batch_true.tolist())\n",
    "\n",
    "            val_loop.set_postfix(acc=batch_corr/batch_total * 100., loss=batch_loss, f1=f1)\n",
    "\n",
    "        print(f\"Val Epoch {e+1} loss={val_loss/len(test_dataloader)} acc={val_corr/val_total * 100} f1={f1_score(val_true, val_preds, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "262e77a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 loss=0.31728164698989664 acc=90.92238642632468 f1=0.900732996794532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/86 [00:01<?, ?it/s, acc=91.5, f1=0.412, loss=0.303]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 0 loss=0.2591884900317636 acc=92.59051825639786 f1=0.921351201632474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 loss=0.2906424852668262 acc=91.74808178848815 f1=0.9111680432421218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/86 [00:01<?, ?it/s, acc=92, f1=0.429, loss=0.275]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 1 loss=0.2375396994310756 acc=93.24541919387995 f1=0.9286728527694147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 loss=0.26774571258880075 acc=92.47071909013496 f1=0.9196284950723157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/86 [00:01<?, ?it/s, acc=92.7, f1=0.447, loss=0.251]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 2 loss=0.22309148744788282 acc=93.72275223869198 f1=0.9340931831578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 loss=0.2485033975089671 acc=93.08168218505469 f1=0.9266667654721753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/86 [00:01<?, ?it/s, acc=93.4, f1=0.483, loss=0.234]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 3 loss=0.20792901931807053 acc=94.01678939429618 f1=0.9378462032257242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 loss=0.23263662888622674 acc=93.56351498482104 f1=0.9321187519208465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/86 [00:01<?, ?it/s, acc=93.8, f1=0.502, loss=0.213]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 4 loss=0.19837765066429627 acc=94.37638028805459 f1=0.9419144090111531\n"
     ]
    }
   ],
   "source": [
    "train(5, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f37f5",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c80de240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lstm import LSTMNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "770b4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"n_classes\": len(tag_2_idx),\n",
    "    \"n_embeddings\": len(word_2_idx),\n",
    "    \"embed_dims\": 50,\n",
    "    \"n_lstm_layers\": 2,\n",
    "    \"lstm_dims\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1269eb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNER(**model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2036b8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMNER(\n",
       "  (embedding_layer): Embedding(30173, 50)\n",
       "  (lstm_layers): LSTM(50, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (output_layer): Linear(in_features=256, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6c83abc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d43523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 loss=0.6473111892577077 acc=85.97481623550502 f1=0.8138471382900698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/86 [00:13<?, ?it/s, acc=88.8, f1=0.319, loss=0.423]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 1 loss=0.3863755727923194 acc=89.6660578018495 f1=0.8736550118635578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 2 loss=0.3016435936337612 acc=91.9036200122864 f1=0.9092482044492394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/86 [00:14<?, ?it/s, acc=93.5, f1=0.529, loss=0.247]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 2 loss=0.2482122607355894 acc=93.31033648797438 f1=0.9282904077231984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 3 loss=0.2102910633519536 acc=94.16921887361232 f1=0.9375413813972469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/86 [00:13<?, ?it/s, acc=95, f1=0.575, loss=0.189]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Epoch 3 loss=0.19839633221543113 acc=94.46739178859875 f1=0.9420542942193333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 4 loss=0.16648826364916366 acc=95.21759126121357 f1=0.949519244822825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/86 [00:14<?, ?it/s, acc=94.1, f1=0.663, loss=0.219]"
     ]
    }
   ],
   "source": [
    "train(10, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0276b",
   "metadata": {},
   "source": [
    "## CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc864c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
